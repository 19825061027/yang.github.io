#+INCLUDE: "../../prefix.org"

#+TITLE: mmap系统调用回溯

Reference: https://blog.csdn.net/qq_41687938/article/details/119901916
知，
1. 用户看到的虚拟地址空间vm_area_struct
2. 实际物理内存地址空间
3. 实际磁盘物理地址空间
   
mmap的过程只实现了:
虚拟地址空间 -- 磁盘物理地址空间 的联系

缺页引发的异常实现了：
虚拟地址空间     -- 物理内存地址空间 的联系
磁盘物理地址空间 -- 物理内存地址空间 的联系

userspace::mmap
    --> sys_mmap2(unsigned long addr, unsigned long len, unsigned long prot,
                  unsigned long flags,unsigned long fd, unsigned long pgoff);
           --> ksys_mmap_pgoff(unsigned long addr, unsigned long len, unsigned long prot,
                  unsigned long flags,unsigned long fd, unsigned long pgoff);
		  --> vm_mmap_pgoff(unsigned long addr, unsigned long len, unsigned long prot,
                          unsigned long flags,unsigned long fd, unsigned long pgoff);
		          --> do_mmap(struct file *file, unsigned long addr,
			             unsigned long len, unsigned long prot,
			             unsigned long flags, unsigned long pgoff,
			             unsigned long *populate, struct list_head *uf)
				     --> mmap_region(struct file *file, unsigned long addr,
		                                    unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
		                                    struct list_head *uf)
#+begin_src c
    unsigned long mmap_region(struct file *file, unsigned long addr,
		  unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
		  struct list_head *uf)
  {
	  struct mm_struct *mm = current->mm;
	  struct vm_area_struct *vma = NULL;
	  struct vm_area_struct *next, *prev, *merge;
	  pgoff_t pglen = len >> PAGE_SHIFT;
	  unsigned long charged = 0;
	  unsigned long end = addr + len;
	  unsigned long merge_start = addr, merge_end = end;
	  pgoff_t vm_pgoff;
	  int error;
	  VMA_ITERATOR(vmi, mm, addr);

	  /* 先将已经映射到虚拟地址空间 addr --> addr+len的文件部分解除映射 */
	  if (do_vmi_munmap(&vmi, mm, addr, len, uf, false))
		  return -ENOMEM;

	  next = vma_next(&vmi);
	  prev = vma_prev(&vmi);
	  if (vm_flags & VM_SPECIAL)
		  goto cannot_expand;

	  /* 尝试将需要映射部分，展开到前一个或后一个vm_area_struct实例中，如果展开成功就不用再分配新的实例了 */
	  /* Check next */
	  if (next && next->vm_start == end && !vma_policy(next) &&
	      can_vma_merge_before(next, vm_flags, NULL, file, pgoff+pglen,
				   NULL_VM_UFFD_CTX, NULL)) {
		  merge_end = next->vm_end;
		  vma = next;
		  vm_pgoff = next->vm_pgoff - pglen;
	  }

	  /* Check prev */
	  if (prev && prev->vm_end == addr && !vma_policy(prev) &&
	      (vma ? can_vma_merge_after(prev, vm_flags, vma->anon_vma, file,
					 pgoff, vma->vm_userfaultfd_ctx, NULL) :
		     can_vma_merge_after(prev, vm_flags, NULL, file, pgoff,
					 NULL_VM_UFFD_CTX, NULL))) {
		  merge_start = prev->vm_start;
		  vma = prev;
		  vm_pgoff = prev->vm_pgoff;
	  }


	  /* Actually expand, if possible */
	  if (vma &&
	      !vma_expand(&vmi, vma, merge_start, merge_end, vm_pgoff, next)) {
		  khugepaged_enter_vma(vma, vm_flags);
		  goto expanded;
	  }

  cannot_expand:
	  if (prev)
		  vma_iter_next_range(&vmi);

	  /*
	   ,* Determine the object being mapped and call the appropriate
	   ,* specific mapper. the address has already been validated, but
	   ,* not unmapped, but the maps are removed from the list.
	   ,*/
	  vma = vm_area_alloc(mm);
	  if (!vma) {
		  error = -ENOMEM;
		  goto unacct_error;
	  }

	  vma_iter_set(&vmi, addr);
	  vma->vm_start = addr;
	  vma->vm_end = end;
	  vm_flags_init(vma, vm_flags);
	  vma->vm_page_prot = vm_get_page_prot(vm_flags);
	  vma->vm_pgoff = pgoff;

	  if (file) {
		  if (vm_flags & VM_SHARED) {
			  error = mapping_map_writable(file->f_mapping);
			  if (error)
				  goto free_vma;
		  }

		  vma->vm_file = get_file(file);
		  error = file->f_op->mmap(file, vma); // IMPORTANT!! 在此处调用file_ops::mmap,通过虚拟文件系统inode模块定位到文件磁盘物理地址，通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系；此时这片虚拟地址并没有任何数据关联到主存中；
		  // 也就是调用完file_ops::mmap之后，虚拟地址空间 -- 物理内存地址空间还没有建立联系
		  // 而是虚拟地址空间 -- 磁盘物理地址建立了联系
		  // 待之后访问时，再通过上述联系，建立 磁盘物理地址 -- 物理内存地址 的联系，以及磁盘物理地址 -- 虚拟地址空间之间的联系
		  if (error)
			  goto unmap_and_free_vma;

		  /*
		   ,* Expansion is handled above, merging is handled below.
		   ,* Drivers should not alter the address of the VMA.
		   ,*/
		  error = -EINVAL;
		  if (WARN_ON((addr != vma->vm_start)))
			  goto close_and_free_vma;

		  vma_iter_set(&vmi, addr);
		  /*
		   ,* If vm_flags changed after call_mmap(), we should try merge
		   ,* vma again as we may succeed this time.
		   ,*/
		  if (unlikely(vm_flags != vma->vm_flags && prev)) {
			  merge = vma_merge(&vmi, mm, prev, vma->vm_start,
				      vma->vm_end, vma->vm_flags, NULL,
				      vma->vm_file, vma->vm_pgoff, NULL,
				      NULL_VM_UFFD_CTX, NULL);
			  if (merge) {
				  /*
				   ,* ->mmap() can change vma->vm_file and fput
				   ,* the original file. So fput the vma->vm_file
				   ,* here or we would add an extra fput for file
				   ,* and cause general protection fault
				   ,* ultimately.
				   ,*/
				  fput(vma->vm_file);
				  vm_area_free(vma);
				  vma = merge;
				  /* Update vm_flags to pick up the change. */
				  vm_flags = vma->vm_flags;
				  goto unmap_writable;
			  }
		  }

		  vm_flags = vma->vm_flags;
	  } else if (vm_flags & VM_SHARED) {
		  error = shmem_zero_setup(vma);
		  if (error)
			  goto free_vma;
	  } else {
		  vma_set_anonymous(vma);
	  }

	  if (map_deny_write_exec(vma, vma->vm_flags)) {
		  error = -EACCES;
		  goto close_and_free_vma;
	  }

	  /* Allow architectures to sanity-check the vm_flags */
	  error = -EINVAL;
	  if (!arch_validate_flags(vma->vm_flags))
		  goto close_and_free_vma;

	  error = -ENOMEM;
	  if (vma_iter_prealloc(&vmi))
		  goto close_and_free_vma;

	  /* Lock the VMA since it is modified after insertion into VMA tree */
	  vma_start_write(vma);
	  if (vma->vm_file)
		  i_mmap_lock_write(vma->vm_file->f_mapping);

	  vma_iter_store(&vmi, vma);
	  mm->map_count++;
	  if (vma->vm_file) {
		  if (vma->vm_flags & VM_SHARED)
			  mapping_allow_writable(vma->vm_file->f_mapping);

		  flush_dcache_mmap_lock(vma->vm_file->f_mapping);
		  vma_interval_tree_insert(vma, &vma->vm_file->f_mapping->i_mmap);
		  flush_dcache_mmap_unlock(vma->vm_file->f_mapping);
		  i_mmap_unlock_write(vma->vm_file->f_mapping);
	  }

	  /*
	   ,* vma_merge() calls khugepaged_enter_vma() either, the below
	   ,* call covers the non-merge case.
	   ,*/
	  khugepaged_enter_vma(vma, vma->vm_flags);

	  /* Once vma denies write, undo our temporary denial count */
  unmap_writable:
	  if (file && vm_flags & VM_SHARED)
		  mapping_unmap_writable(file->f_mapping);
	  file = vma->vm_file;
	  ksm_add_vma(vma);
  expanded:
	  perf_event_mmap(vma);

	  vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);
	  if (vm_flags & VM_LOCKED) {
		  if ((vm_flags & VM_SPECIAL) || vma_is_dax(vma) ||
					  is_vm_hugetlb_page(vma) ||
					  vma == get_gate_vma(current->mm))
			  vm_flags_clear(vma, VM_LOCKED_MASK);
		  else
			  mm->locked_vm += (len >> PAGE_SHIFT);
	  }

	  if (file)
		  uprobe_mmap(vma);

	  /*
	   ,* New (or expanded) vma always get soft dirty status.
	   ,* Otherwise user-space soft-dirty page tracker won't
	   ,* be able to distinguish situation when vma area unmapped,
	   ,* then new mapped in-place (which must be aimed as
	   ,* a completely new data area).
	   ,*/
	  vm_flags_set(vma, VM_SOFTDIRTY);

	  vma_set_page_prot(vma);

	  validate_mm(mm);
	  return addr;

  close_and_free_vma:
	  if (file && vma->vm_ops && vma->vm_ops->close)
		  vma->vm_ops->close(vma);

	  if (file || vma->vm_file) {
  unmap_and_free_vma:
		  fput(vma->vm_file);
		  vma->vm_file = NULL;

		  /* Undo any partial mapping done by a device driver. */
		  unmap_region(mm, &mm->mm_mt, vma, prev, next, vma->vm_start,
			       vma->vm_end, true);
	  }
	  if (file && (vm_flags & VM_SHARED))
		  mapping_unmap_writable(file->f_mapping);
  free_vma:
	  vm_area_free(vma);
  unacct_error:
	  if (charged)
		  vm_unacct_memory(charged);
	  validate_mm(mm);
	  return error;
  }

#+end_src
